# -*- coding: utf-8 -*-
"""–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1msREAqoauCUd00XbQzkGR9YLzHhQIG9j
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, LabelBinarizer


# %matplotlib inline

from google.colab import drive

drive.mount('/content/drive/')

df = pd.read_csv('/content/drive/MyDrive/–ñ–ö–•/–≥–æ—Ç–æ–≤–æ.csv')

!pip install emoji
import emoji

for index, row in enumerate(df['—Ç–µ–∫—Å—Ç']):
  df['—Ç–µ–∫—Å—Ç'] = df['—Ç–µ–∫—Å—Ç'].replace(df['—Ç–µ–∫—Å—Ç'][index], (emoji.demojize(df['—Ç–µ–∫—Å—Ç'][index], language='ru')))

category = df['–∫–∞—Ç–µ–≥–æ—Ä–∏—è'].value_counts()
emotion = df['—ç–º–æ—Ü–∏—è'].value_counts()
urgency = df['—Å—Ä–æ—á–Ω–æ—Å—Ç—å'].value_counts()

print("–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞:", len(df))
print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π:")
print(category)
print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —ç–º–æ—Ü–∏–π:")
print(emotion)
print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ä–æ—á–Ω–æ—Å—Ç–∏:")
print(urgency)

from matplotlib import rcParams

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∫–∞ —Å—Ç–∏–ª—è
plt.style.use('seaborn-v0_8')
rcParams['font.family'] = 'DejaVu Sans'
rcParams['font.size'] = 16
rcParams['figure.titlesize'] = 20
rcParams['axes.titlesize'] = 14
rcParams['axes.labelsize'] = 18

def plot_main_group_distribution(main_group_series, figsize=(16, 10)):
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –≥—Ä—É–ø–ø –∂–∞–ª–æ–±

    Parameters:
    main_group_series: pd.Series —Å value_counts() –¥–ª—è main_group
    figsize: tuple - —Ä–∞–∑–º–µ—Ä —Ñ–∏–≥—É—Ä—ã
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)

    russian_names = {
        'water_systems': '–í–æ–¥–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã',
        'electricity': '–≠–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–æ',
        'heating_ventilation': '–û—Ç–æ–ø–ª–µ–Ω–∏–µ –∏ –≤–µ–Ω—Ç–∏–ª—è—Ü–∏—è',
        'elevators': '–õ–∏—Ñ—Ç—ã',
        'waste_cleaning': '–ú—É—Å–æ—Ä –∏ —É–±–æ—Ä–∫–∞',
        'building_structures': '–ö–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∑–¥–∞–Ω–∏—è',
        'amenities': '–ë–ª–∞–≥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ',
        'safety_security': '–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å'
    }
    main_group_russian = main_group_series.rename(index=russian_names)

    # Bar plot
    colors = plt.cm.Set3(np.linspace(0, 1, len(main_group_russian)))
    bars = ax1.bar(range(len(main_group_russian)), main_group_russian.values, color=colors, alpha=0.8)

    ax1.set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∂–∞–ª–æ–± –ø–æ –æ—Å–Ω–æ–≤–Ω—ã–º –≥—Ä—É–ø–ø–∞–º\n(—Å—Ç–æ–ª–±—á–∞—Ç–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞)',
                  fontsize=16, fontweight='bold', pad=20)
    ax1.set_xlabel('–û—Å–Ω–æ–≤–Ω—ã–µ –≥—Ä—É–ø–ø—ã', fontsize=14, fontweight='bold')
    ax1.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∂–∞–ª–æ–±', fontsize=14, fontweight='bold')


    ax1.set_xticks(range(len(main_group_russian)))
    ax1.set_xticklabels(main_group_russian.index, rotation=45, ha='right', fontsize=12)

    for i, bar in enumerate(bars):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + max(main_group_russian.values)*0.01,
                f'{int(height)}', ha='center', va='bottom', fontsize=11, fontweight='bold')

    ax1.grid(axis='y', alpha=0.3, linestyle='--')
    ax1.set_axisbelow(True)

    # Pie chart
    explode = [0.05 if value < max(main_group_russian.values) * 0.1 else 0 for value in main_group_russian.values]

    wedges, texts, autotexts = ax2.pie(main_group_russian.values,
                                      labels=main_group_russian.index,
                                      autopct='%1.1f%%',
                                      startangle=90,
                                      colors=colors,
                                      explode=explode,
                                      shadow=True,
                                      textprops={'fontsize': 11})

    for autotext in autotexts:
        autotext.set_color('black')
        autotext.set_fontweight('bold')
        autotext.set_fontsize(12)

    for text in texts:
        text.set_fontsize(11)

    ax2.set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∂–∞–ª–æ–± –ø–æ –æ—Å–Ω–æ–≤–Ω—ã–º –≥—Ä—É–ø–ø–∞–º\n(–∫—Ä—É–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞)',
                  fontsize=16, fontweight='bold', pad=20)

    fig.subplots_adjust(top=0.85)
    plt.tight_layout(rect=[0, 0, 1, 0.85])
    plt.subplots_adjust(top=0.9)

    print("\n" + "="*60)
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –û–°–ù–û–í–ù–´–ú –ì–†–£–ü–ü–ê–ú:")
    print("="*60)
    total_complaints = main_group_series.sum()
    for i, (group, count) in enumerate(main_group_russian.items(), 1):
        percentage = (count / total_complaints) * 100
        print(f"{i:2d}. {group:25} | {count:4d} –∂–∞–ª–æ–± | {percentage:5.1f}%")

    print(f"\n–í—Å–µ–≥–æ –∂–∞–ª–æ–±: {total_complaints}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(main_group_series)}")

plot_main_group_distribution(category)
plot_main_group_distribution(emotion)
plot_main_group_distribution(urgency)

!pip install iterative-stratification

from iterstrat.ml_stratifiers import MultilabelStratifiedKFold

def prepare_multilabel_stratification(df, test_size=0.2, val_size=0.1, random_state=42):
    lb_category = LabelBinarizer()
    lb_emotion = LabelBinarizer()
    lb_urgency = LabelBinarizer()

    category_onehot = lb_category.fit_transform(df['–∫–∞—Ç–µ–≥–æ—Ä–∏—è'])
    emotion_onehot = lb_emotion.fit_transform(df['—ç–º–æ—Ü–∏—è'])
    urgency_onehot = lb_urgency.fit_transform(df['—Å—Ä–æ—á–Ω–æ—Å—Ç—å'])
    y_multitask = np.concatenate([category_onehot, emotion_onehot, urgency_onehot], axis=1)

    print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞: {y_multitask.shape}")
    print(f"–ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {lb_category.classes_}")
    print(f"–≠–º–æ—Ü–∏–∏: {lb_emotion.classes_}")
    print(f"–°—Ä–æ—á–Ω–æ—Å—Ç—å: {lb_urgency.classes_}")

    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)

    indices = np.arange(len(df))
    for train_val_idx, test_idx in mskf.split(indices, y_multitask):
        train_val_df = df.iloc[train_val_idx]
        test_df = df.iloc[test_idx]
        break

    category_onehot_val = lb_category.transform(train_val_df['–∫–∞—Ç–µ–≥–æ—Ä–∏—è'])
    emotion_onehot_val = lb_emotion.transform(train_val_df['—ç–º–æ—Ü–∏—è'])
    urgency_onehot_val = lb_urgency.transform(train_val_df['—Å—Ä–æ—á–Ω–æ—Å—Ç—å'])

    y_multitask_val = np.concatenate([category_onehot_val, emotion_onehot_val, urgency_onehot_val], axis=1)

    train_val_indices = np.arange(len(train_val_df))
    for train_idx, val_idx in mskf.split(train_val_indices, y_multitask_val):
        train_df = train_val_df.iloc[train_idx]
        val_df = train_val_df.iloc[val_idx]
        break

    result = {
        'train': train_df,
        'val': val_df,
        'test': test_df,
        'encoders': {
            'category': lb_category,
            'emotion': lb_emotion,
            'urgency': lb_urgency
        }
    }

    return result

stratification_result = prepare_multilabel_stratification(df)

def check_stratification_quality(result):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏"""

    train_df = result['train']
    val_df = result['val']
    test_df = result['test']

    print("=" * 50)
    print("–ü–†–û–í–ï–†–ö–ê –°–¢–†–ê–¢–ò–§–ò–ö–ê–¶–ò–ò")
    print("=" * 50)

    for split_name, split_df in [('TRAIN', train_df), ('VAL', val_df), ('TEST', test_df)]:
        print(f"\n{split_name} ({len(split_df)} –∑–∞–ø–∏—Å–µ–π):")

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
        cat_dist = split_df['–∫–∞—Ç–µ–≥–æ—Ä–∏—è'].value_counts(normalize=True)
        print("  –ö–∞—Ç–µ–≥–æ—Ä–∏–∏:")
        for cat, perc in cat_dist.items():
            print(f"    {cat}: {perc:.1%}")

        # –≠–º–æ—Ü–∏–∏
        emo_dist = split_df['—ç–º–æ—Ü–∏—è'].value_counts(normalize=True)
        print("  –≠–º–æ—Ü–∏–∏:")
        for emo, perc in emo_dist.items():
            print(f"    {emo}: {perc:.1%}")

        # –°—Ä–æ—á–Ω–æ—Å—Ç—å
        urg_dist = split_df['—Å—Ä–æ—á–Ω–æ—Å—Ç—å'].value_counts(normalize=True)
        print("  –°—Ä–æ—á–Ω–æ—Å—Ç—å:")
        for urg, perc in urg_dist.items():
            print(f"    {urg}: {perc:.1%}")

check_stratification_quality(stratification_result)

stratified_category = stratification_result['train']['–∫–∞—Ç–µ–≥–æ—Ä–∏—è'].value_counts()
stratified_emotion = stratification_result['train']['—ç–º–æ—Ü–∏—è'].value_counts()
stratified_urgency = stratification_result['train']['—Å—Ä–æ—á–Ω–æ—Å—Ç—å'].value_counts()

plot_main_group_distribution(stratified_category)
plot_main_group_distribution(stratified_emotion)
plot_main_group_distribution(stratified_urgency)

import torch
from transformers import AutoTokenizer
from torch.utils.data import Dataset, DataLoader

class MultiTaskDataset(Dataset):
    def __init__(self, texts, categories, emotions, urgencies, tokenizer, max_length=512):
        self.texts = texts
        self.categories = categories
        self.emotions = emotions
        self.urgencies = urgencies
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __getitem__(self, idx):
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        encoding = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –∫–ª–∞—Å—Å–æ–≤ –∏–∑ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–æ–≤
        item = {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'category_labels': torch.tensor(self.categories[idx]),
            'emotion_labels': torch.tensor(self.emotions[idx]),
            'urgency_labels': torch.tensor(self.urgencies[idx])
        }
        return item

    def __len__(self):
        return len(self.texts)

def prepare_datasets(result, tokenizer_name='cointegrated/rubert-tiny2'):
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""

    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

    lb_category = result['encoders']['category']
    lb_emotion = result['encoders']['emotion']
    lb_urgency = result['encoders']['urgency']

    def prepare_split(df):
        category_indices = lb_category.transform(df['–∫–∞—Ç–µ–≥–æ—Ä–∏—è']).argmax(axis=1)
        emotion_indices = lb_emotion.transform(df['—ç–º–æ—Ü–∏—è']).argmax(axis=1)
        urgency_indices = lb_urgency.transform(df['—Å—Ä–æ—á–Ω–æ—Å—Ç—å']).argmax(axis=1)

        return MultiTaskDataset(
            texts=df['—Ç–µ–∫—Å—Ç'].tolist(),
            categories=category_indices,
            emotions=emotion_indices,
            urgencies=urgency_indices,
            tokenizer=tokenizer
        )

    train_dataset = prepare_split(result['train'])
    val_dataset = prepare_split(result['val'])
    test_dataset = prepare_split(result['test'])

    return {
        'train': train_dataset,
        'val': val_dataset,
        'test': test_dataset,
        'encoders': result['encoders'],
        'num_categories': len(lb_category.classes_),
        'num_emotions': len(lb_emotion.classes_),
        'num_urgencies': len(lb_urgency.classes_)
    }

datasets = prepare_datasets(stratification_result)

import torch.nn as nn
from transformers import BertPreTrainedModel, BertModel

class SimpleMultiTaskBERT(BertPreTrainedModel):
    def __init__(self, config, num_categories, num_emotions, num_urgencies):
        super().__init__(config)
        self.bert = BertModel(config)
        self.dropout = nn.Dropout(0.8)

        self.category_dropout = nn.Dropout(0.9)

        self.classifier_category = nn.Linear(config.hidden_size, num_categories)
        self.classifier_emotion = nn.Linear(config.hidden_size, num_emotions)
        self.classifier_urgency = nn.Linear(config.hidden_size, num_urgencies)

    def forward(self, input_ids=None, attention_mask=None):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        pooled_output = outputs[1]

        category_output = self.category_dropout(pooled_output)
        emotion_output = self.dropout(pooled_output)
        urgency_output = self.dropout(pooled_output)

        category_logits = self.classifier_category(category_output)
        emotion_logits = self.classifier_emotion(emotion_output)
        urgency_logits = self.classifier_urgency(urgency_output)

        return {
            'category_logits': category_logits,
            'emotion_logits': emotion_logits,
            'urgency_logits': urgency_logits
        }

from transformers import AutoConfig

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
model_name = 'cointegrated/rubert-tiny2'
config = AutoConfig.from_pretrained(model_name)

model = SimpleMultiTaskBERT.from_pretrained(
    model_name,
    config=config,
    num_categories=datasets['num_categories'],
    num_emotions=datasets['num_emotions'],
    num_urgencies=datasets['num_urgencies']
)

print(f"–ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:")
print(f"- –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {datasets['num_categories']} –∫–ª–∞—Å—Å–æ–≤")
print(f"- –≠–º–æ—Ü–∏–∏: {datasets['num_emotions']} –∫–ª–∞—Å—Å–æ–≤")
print(f"- –°—Ä–æ—á–Ω–æ—Å—Ç—å: {datasets['num_urgencies']} –∫–ª–∞—Å—Å–æ–≤")

from transformers import AutoConfig, get_linear_schedule_with_warmup
import torch
from torch.utils.data import DataLoader


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

train_dataloader = DataLoader(datasets['train'], batch_size=16, shuffle=True)
valid_dataloader = DataLoader(datasets['val'], batch_size=16)    # validation
test_dataloader = DataLoader(datasets['test'], batch_size=16)    # test (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)

optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=1e-5,
    weight_decay=0.01
)

total_steps = len(train_dataloader) * 10
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=100,
    num_training_steps=total_steps
)

def compute_accuracy(logits, labels):
    predictions = torch.argmax(logits, dim=1)
    correct = (predictions == labels).float().sum()
    return correct / len(labels)

def compute_balanced_loss(outputs, category_labels, emotion_labels, urgency_labels):
    loss_fct = torch.nn.CrossEntropyLoss()

    loss_category = loss_fct(outputs['category_logits'], category_labels)
    loss_emotion = loss_fct(outputs['emotion_logits'], emotion_labels)
    loss_urgency = loss_fct(outputs['urgency_logits'], urgency_labels)

    category_probs = torch.softmax(outputs['category_logits'], dim=1)
    category_entropy = -torch.sum(category_probs * torch.log(category_probs + 1e-8), dim=1).mean()

    entropy_penalty = -0.1 * category_entropy

    return loss_category + loss_emotion + loss_urgency + entropy_penalty

model.train()
best_val_loss = float('inf')
patience = 3
patience_counter = 0

for epoch in range(8):
    total_loss = 0
    total_category_acc = 0
    total_emotion_acc = 0
    total_urgency_acc = 0

    for batch_idx, batch in enumerate(train_dataloader):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        category_labels = batch['category_labels'].to(device)  # –ò–ó–ú–ï–ù–ò–õ–ò –° main –Ω–∞ category
        emotion_labels = batch['emotion_labels'].to(device)
        urgency_labels = batch['urgency_labels'].to(device)

        optimizer.zero_grad()

        # Forward pass (–ë–µ–∑ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è loss –≤ –º–æ–¥–µ–ª–∏)
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask

        )

        # loss —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π
        loss = compute_balanced_loss(outputs, category_labels, emotion_labels, urgency_labels)

        loss.backward()
        for name, param in model.named_parameters():
            if 'classifier_category' in name and param.grad is not None:
                param.grad = param.grad * 0.5
        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
        optimizer.step()
        scheduler.step()

        # –º–µ—Ç—Ä–∏–∫–∏
        total_loss += loss.item()
        total_category_acc += compute_accuracy(outputs['category_logits'], category_labels).item()
        total_emotion_acc += compute_accuracy(outputs['emotion_logits'], emotion_labels).item()
        total_urgency_acc += compute_accuracy(outputs['urgency_logits'], urgency_labels).item()

        if batch_idx % 50 == 0:
            avg_loss = total_loss / (batch_idx + 1)
            avg_category_acc = total_category_acc / (batch_idx + 1)
            avg_emotion_acc = total_emotion_acc / (batch_idx + 1)
            avg_urgency_acc = total_urgency_acc / (batch_idx + 1)

            print(f'Epoch: {epoch+1}, Batch: {batch_idx}, '
                  f'Loss: {avg_loss:.4f}, '
                  f'Category Acc: {avg_category_acc:.4f}, '
                  f'Emotion Acc: {avg_emotion_acc:.4f}, '
                  f'Urgency Acc: {avg_urgency_acc:.4f}')

    # –í–ê–õ–ò–î–ê–¶–ò–Ø
    model.eval()
    val_loss = 0
    val_category_acc = 0
    val_emotion_acc = 0
    val_urgency_acc = 0

    with torch.no_grad():
        for val_batch in valid_dataloader:
            input_ids = val_batch['input_ids'].to(device)
            attention_mask = val_batch['attention_mask'].to(device)
            category_labels = val_batch['category_labels'].to(device)
            emotion_labels = val_batch['emotion_labels'].to(device)
            urgency_labels = val_batch['urgency_labels'].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )

            # loss –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            loss = compute_balanced_loss(outputs, category_labels, emotion_labels, urgency_labels)

            val_loss += loss.item()
            val_category_acc += compute_accuracy(outputs['category_logits'], category_labels).item()
            val_emotion_acc += compute_accuracy(outputs['emotion_logits'], emotion_labels).item()
            val_urgency_acc += compute_accuracy(outputs['urgency_logits'], urgency_labels).item()

    val_loss /= len(valid_dataloader)
    val_category_acc /= len(valid_dataloader)
    val_emotion_acc /= len(valid_dataloader)
    val_urgency_acc /= len(valid_dataloader)

    print(f'\n=== Epoch {epoch+1} Validation ===')
    print(f'Val Loss: {val_loss:.4f}')
    print(f'Val Category Acc: {val_category_acc:.4f}')
    print(f'Val Emotion Acc: {val_emotion_acc:.4f}')
    print(f'Val Urgency Acc: {val_urgency_acc:.4f}\n')

    # –†–ê–ù–ù–Ø–Ø –û–°–¢–ê–ù–û–í–ö–ê
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        # –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å
        torch.save(model.state_dict(), 'best_model.pth')
        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å val_loss: {val_loss:.4f}")
    else:
        patience_counter += 1
        print(f"‚ö†Ô∏è Patience: {patience_counter}/{patience}")

        if patience_counter >= patience:
            print("üõë –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞!")
            break

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
    model.train()

# –§–ò–ù–ê–õ–¨–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï
print("\n=== –§–ò–ù–ê–õ–¨–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ===")
model.load_state_dict(torch.load('best_model.pth'))
model.eval()

test_category_acc = 0
test_emotion_acc = 0
test_urgency_acc = 0

with torch.no_grad():
    for test_batch in test_dataloader:
        input_ids = test_batch['input_ids'].to(device)
        attention_mask = test_batch['attention_mask'].to(device)
        category_labels = test_batch['category_labels'].to(device)
        emotion_labels = test_batch['emotion_labels'].to(device)
        urgency_labels = test_batch['urgency_labels'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)

        test_category_acc += compute_accuracy(outputs['category_logits'], category_labels).item()
        test_emotion_acc += compute_accuracy(outputs['emotion_logits'], emotion_labels).item()
        test_urgency_acc += compute_accuracy(outputs['urgency_logits'], urgency_labels).item()

test_category_acc /= len(test_dataloader)
test_emotion_acc /= len(test_dataloader)
test_urgency_acc /= len(test_dataloader)

print(f"üéØ –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ TEST –¥–∞–Ω–Ω—ã—Ö:")
print(f"Test Category Acc: {test_category_acc:.4f}")
print(f"Test Emotion Acc: {test_emotion_acc:.4f}")
print(f"Test Urgency Acc: {test_urgency_acc:.4f}")

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
import torch

def compute_metrics_for_task(logits, labels, num_classes):
    """–í—ã—á–∏—Å–ª—è–µ—Ç –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–∏"""
    predictions = torch.argmax(logits, dim=1).cpu().numpy()
    probabilities = torch.softmax(logits, dim=1).cpu().numpy()
    true_labels = labels.cpu().numpy()

    # –î–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º macro averaging
    if num_classes > 2:
        precision = precision_score(true_labels, predictions, average='macro', zero_division=0)
        recall = recall_score(true_labels, predictions, average='macro', zero_division=0)
        f1 = f1_score(true_labels, predictions, average='macro', zero_division=0)
        try:
            roc_auc = roc_auc_score(true_labels, probabilities, multi_class='ovr', average='macro')
        except:
            roc_auc = 0.0
    else:
        # –î–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        precision = precision_score(true_labels, predictions, average='binary', zero_division=0)
        recall = recall_score(true_labels, predictions, average='binary', zero_division=0)
        f1 = f1_score(true_labels, predictions, average='binary', zero_division=0)
        try:
            roc_auc = roc_auc_score(true_labels, probabilities[:, 1])
        except:
            roc_auc = 0.0

    return {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'roc_auc': roc_auc
    }

def plot_simple_metrics_comparison(model, test_dataloader, device):
    """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ—Å—Ç–æ–π –≥—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫"""

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    model.eval()
    all_outputs = {
        'category': {'logits': [], 'labels': []},
        'emotion': {'logits': [], 'labels': []},
        'urgency': {'logits': [], 'labels': []}
    }

    with torch.no_grad():
        for batch in test_dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)

            all_outputs['category']['logits'].append(outputs['category_logits'])
            all_outputs['category']['labels'].append(batch['category_labels'].to(device))
            all_outputs['emotion']['logits'].append(outputs['emotion_logits'])
            all_outputs['emotion']['labels'].append(batch['emotion_labels'].to(device))
            all_outputs['urgency']['logits'].append(outputs['urgency_logits'])
            all_outputs['urgency']['labels'].append(batch['urgency_labels'].to(device))

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ –≤—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    metrics = {}
    task_names = ['–ö–∞—Ç–µ–≥–æ—Ä–∏—è', '–≠–º–æ—Ü–∏—è', '–°—Ä–æ—á–Ω–æ—Å—Ç—å']
    task_keys = ['category', 'emotion', 'urgency']
    num_classes_map = {
        'category': 11,
        'emotion': 6,
        'urgency': 5
    }

    for task_key, task_name in zip(task_keys, task_names):
        logits = torch.cat(all_outputs[task_key]['logits'])
        labels = torch.cat(all_outputs[task_key]['labels'])
        metrics[task_name] = compute_metrics_for_task(logits, labels, num_classes_map[task_key])

    fig, ax = plt.subplots(figsize=(12, 8))

    metric_names = ['Precision', 'Recall', 'F1-score', 'ROC-AUC']
    x_pos = np.arange(len(metric_names))

    width = 0.25

    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']

    for i, (task_name, color) in enumerate(zip(task_names, colors)):
        task_metrics = metrics[task_name]
        values = [
            task_metrics['precision'],
            task_metrics['recall'],
            task_metrics['f1'],
            task_metrics['roc_auc']
        ]

        bars = ax.bar(x_pos + i * width, values, width,
                     label=task_name, color=color, alpha=0.8)

        for bar, value in zip(bars, values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

    ax.set_xlabel('–ú–µ—Ç—Ä–∏–∫–∏', fontsize=12, fontweight='bold')
    ax.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏—è', fontsize=12, fontweight='bold')
    ax.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø–æ –∑–∞–¥–∞—á–∞–º', fontsize=14, fontweight='bold')
    ax.set_xticks(x_pos + width)
    ax.set_xticklabels(metric_names)
    ax.set_ylim(0, 1.1)
    ax.legend()
    ax.grid(True, alpha=0.3, axis='y')

    # –î–æ–±–∞–≤–ª—è–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—É—é –ª–∏–Ω–∏—é –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='–°–ª—É—á–∞–π–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä')

    plt.tight_layout()
    plt.show()

    print("\nüìä –ß–ò–°–õ–ï–ù–ù–´–ï –ó–ù–ê–ß–ï–ù–ò–Ø –ú–ï–¢–†–ò–ö:")
    print("=" * 60)
    for task_name in task_names:
        print(f"\n{task_name.upper()}:")
        print(f"  Precision: {metrics[task_name]['precision']:.4f}")
        print(f"  Recall:    {metrics[task_name]['recall']:.4f}")
        print(f"  F1-score:  {metrics[task_name]['f1']:.4f}")
        print(f"  ROC-AUC:   {metrics[task_name]['roc_auc']:.4f}")

    return metrics


print("\n" + "="*80)
print("üìà –ü–û–°–¢–†–û–ï–ù–ò–ï –ì–†–ê–§–ò–ö–ê –ú–ï–¢–†–ò–ö")
print("="*80)

model.load_state_dict(torch.load('best_model.pth'))
model.to(device)

test_metrics = plot_simple_metrics_comparison(model, test_dataloader, device)

print("\n‚úÖ –ì—Ä–∞—Ñ–∏–∫ –º–µ—Ç—Ä–∏–∫ –ø–æ—Å—Ç—Ä–æ–µ–Ω!")

import pickle

def save_encoders(encoders, filepath="./tiny2_encoders.pkl"):
    encoder_package = {
        'category': {
            'encoder': encoders['category'],
            'classes': encoders['category'].classes_.tolist()
        },
        'emotion': {
            'encoder': encoders['emotion'],
            'classes': encoders['emotion'].classes_.tolist()
        },
        'urgency': {
            'encoder': encoders['urgency'],
            'classes': encoders['urgency'].classes_.tolist()
        }
    }

    with open(filepath, 'wb') as f:
        pickle.dump(encoder_package, f)

    print("–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ")
    return filepath

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
encoders = stratification_result['encoders']
encoder_path = save_encoders(encoders)

torch.save(model.state_dict(), "./multitask_model_weights_tiny2.pth")